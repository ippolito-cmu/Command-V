# Command-V: Pasting LLM Behaviors via Activation Profiles 
[![arXiv](https://img.shields.io/badge/arXiv-Preprint%20available-red.svg)](https://arxiv.org/abs/2506.19140)

**Finetune once, use on many LLMs.** âŒ˜V is the first to demonstrate that you can "copy-paste" a finetuned adapter (for refusal suppression, jailbreaking resistance, chain-of-thought reasoning, etc.) learned by one LLM to another of a different size or architecture, without backpropagation.

![PDF Page 1](commandv/Overview.png)
### How it works
âŒ˜V leverages the fact that all transformers LLM activations (intermediate representations) can be intervened for behaviors using PEFT and approximated from another LLM. So to get it running:
1. **Profile activations** on both models using a small set of prompts
2. **Create converters** that map between model representation spaces
3. **Transfer behaviors** by applying the donor model's learned interventions to the recipient

See details in our [paper](https://www.arxiv.org/abs/2506.19140).

## âŒ˜V Highlights

- **Low Resource**: No backpropagation. No expert data. Low GPU memory usage. 
- **Fast**: Minutes to set up, not hours.
- **Cross-Family**: In certain conditions, works between Llama, Qwen, and other model families


## ğŸª§ Demo

Check out the complete pipeline ([./command_v_demo.ipynb](https://github.com/GithuBarry/Command-V/blob/main/command_v_demo.ipynb)) or on Colab:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GithuBarry/Command-V/blob/main/command_v_demo.ipynb) 


## ğŸ¯ Quick Start

### Installation

```bash
git clone https://github.com/GithuBarry/Command-V.git
cd Command-V
pip install -r requirements.txt
```

### Three-Step Pipeline
```bash
# Profile models on LIMA dataset (once per model)
python step1_capture_activations.py --models meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.1-8B-Instruct
```
```bash
# Learn activation space mappings (takes seconds)
# (Mappings are created on the fly in practice, making this step optional.)
python step2_derive_converters.py derive \
  --source-model meta-llama/Llama-3.1-8B-Instruct \
  --target-model meta-llama/Llama-3.2-1B-Instruct
```
```bash
# Transfer behaviors during inference
python step3_commandv_inference.py \
  --recipient-model meta-llama/Llama-3.2-1B-Instruct \
  --donor-model meta-llama/Llama-3.1-8B-Instruct \
  --adapter-folder reft-adapters/jailbreak/Llama-3.1-8B-Instruct/NodireftIntervention/l1/walledai--AdvBench/L0;2;4;6;8;10;12;14;16;18;20;22;24;26;28;30 \
  --input-source prompts/AdvBench/test.txt \
  --first-n 5 --print-output-only
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ command_v_demo.ipynb         # Jupyter Demo
â”œâ”€â”€ step1_capture_activations.py # Step 1: Activation profiling
â”œâ”€â”€ step2_derive_converters.py   # Step 2: Converter derivation  
â”œâ”€â”€ step3_commandv_inference.py  # Step 3: Behavioral transfer
â”œâ”€â”€ commandv/                    # Core library
â”‚   â”œâ”€â”€ core/                    # Main functionality
â”‚   â”‚   â”œâ”€â”€ capture.py           # Activation capture
â”‚   â”‚   â”œâ”€â”€ converters.py        # Pseudoinverse converters
â”‚   â”‚   â””â”€â”€ inference.py         # Inference engine
â”‚   â”œâ”€â”€ utils/                   # Utilities
â”‚   â””â”€â”€ data/                    # Data processing
â”œâ”€â”€ outputs/                     # Generated files
â”‚   â”œâ”€â”€ activations/             # Model activation profiles
â”‚   â”œâ”€â”€ converters/              # Converter mappings (not by default)
â”‚   â””â”€â”€ inferences/              # Results
â””â”€â”€ reft-adapters/               # Trained behavior adapters
```


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
Certain artifacts are further gated for their potential to be abused.
